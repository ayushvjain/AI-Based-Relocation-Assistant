{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Sheet: Fenway\n",
      "Available Columns: ['Location', 'NO', 'Address', 'Unit', 'Layout', 'Note', 'Date', 'Rent', 'Utilities included', 'Broker fee', 'Security Deposit', 'Video', 'Numbers of \\nNames on lease', 'Note.1']\n",
      "✔ Columns found in Fenway, processing...\n",
      "Processing Sheet: Brookline\n",
      "Available Columns: ['Location', 'Number', 'Address', 'Unit', 'Layout', 'Note', 'Date', 'Rent', 'Utilities included', 'Broker fee', 'Security Deposit', 'Laundry', 'Video', 'Numbers of \\nNames on lease', 'Note.1']\n",
      "✔ Columns found in Brookline, processing...\n",
      "Processing Sheet: Mission hill\n",
      "Available Columns: ['Location', 'Number', 'Address', 'Unit', 'Layout', 'Note', 'Date', 'Rent', 'Utilities included', 'Broker fee', 'Security Deposit', 'Video/photos', 'Numbers of \\nNames on lease', 'Note.1']\n",
      "✔ Columns found in Mission hill, processing...\n",
      "Processing Sheet: AllstonBrighton\n",
      "Available Columns: ['Location', 'Number', 'Address', 'Unit', 'Layout', 'Note', 'Move in\\nDate', 'Rent', 'Utilities included', 'Broker fee', 'Security Deposit', 'Laundry', 'Video', 'Maximun No. of \\nNames on lease', 'Note.1']\n",
      "✔ Columns found in AllstonBrighton, processing...\n",
      "Processing Sheet: Jamaica Plain\n",
      "Available Columns: ['Location', 'Number', 'Address', 'Unit', 'Layout', 'Note', 'Date', 'Rent', 'Utilities included', 'Broker fee', 'Security Deposit', 'Video', 'Numbers of \\nNames on lease', 'Note.1']\n",
      "✔ Columns found in Jamaica Plain, processing...\n",
      "Processing Sheet: Roxbury\n",
      "Available Columns: ['Location', 'Number', 'Address', 'Unit', 'Layout', 'Note', 'Date', 'Rent', 'Utilities included', 'Broker fee', 'Security Deposit', 'Video', 'Numbers of \\nNames on lease', 'Note.1']\n",
      "✔ Columns found in Roxbury, processing...\n",
      "Processing Sheet: Medford\n",
      "Available Columns: ['Location', 'Number', 'Address', 'Unit', 'Layout', 'Note', 'Date', 'Rent', 'Utilities included', 'Broker fee', 'Security Deposit', 'Video', 'Numbers of \\nNames on lease', 'Note.1']\n",
      "✔ Columns found in Medford, processing...\n",
      "Processing Sheet: DowntownSouth End\n",
      "Available Columns: ['Location', 'Number', 'Address', 'Unit', 'Layout', 'Note', 'Date', 'Rent', 'Utilities included', 'Broker fee', 'Security Deposit', 'Video', 'Numbers of \\nNames on lease', 'Note.1']\n",
      "✔ Columns found in DowntownSouth End, processing...\n",
      "Processing Sheet: Cambridge\n",
      "Available Columns: ['Cambridge\\nUsually very close to the Red Line and bus to NEU\\nThe commute to NEU is a bit further, normally around 35-40 mins. \\nBut to MIT and Harvard this is very convenient.', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13']\n",
      "⚠ Skipping Cambridge due to missing columns!\n",
      "Processing Sheet: Roommate Searching\n",
      "Available Columns: ['If you are looking for a roommate:']\n",
      "⚠ Skipping Roommate Searching due to missing columns!\n",
      "Processing Sheet: Sublease\n",
      "Available Columns: []\n",
      "⚠ Skipping Sublease due to missing columns!\n",
      "✅ CSV file 'boston_apartment_listings.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"2025 Boston rental Listing (1).xlsx\"  # Replace with actual path\n",
    "output_csv = \"boston_apartment_listings.csv\"\n",
    "\n",
    "# Load all sheet names\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Define expected columns\n",
    "columns_needed = [\"Address\", \"Unit\", \"Layout\", \"Rent\"]\n",
    "\n",
    "# Initialize an empty DataFrame to store results\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each sheet\n",
    "for sheet_name in xls.sheet_names:\n",
    "    # Read the sheet\n",
    "    df = pd.read_excel(xls, sheet_name=sheet_name, dtype=str)\n",
    "\n",
    "    # Convert column names to string and strip spaces\n",
    "    df.columns = df.columns.astype(str).str.strip()\n",
    "\n",
    "    # Check available columns\n",
    "    print(f\"Processing Sheet: {sheet_name}\")\n",
    "    print(\"Available Columns:\", df.columns.tolist())\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if all(col in df.columns for col in columns_needed):\n",
    "        print(f\"✔ Columns found in {sheet_name}, processing...\")\n",
    "\n",
    "        # Check which column to use for Address\n",
    "        if \"Number\" in df.columns and df[\"Number\"].notna().any():\n",
    "            df[\"Address\"] = df[\"Number\"].fillna('') + \" \" + df[\"Address\"].fillna('') + \" Unit \" + df[\"Unit\"].fillna('')\n",
    "        elif \"NO\" in df.columns and df[\"NO\"].notna().any():\n",
    "            df[\"Address\"] = df[\"NO\"].fillna('') + \" \" + df[\"Address\"].fillna('') + \" Unit \" + df[\"Unit\"].fillna('')\n",
    "        else:\n",
    "            df[\"Address\"] = df[\"Address\"].fillna('') + \" Unit \" + df[\"Unit\"].fillna('')  # Fallback if Number/NO is missing\n",
    "\n",
    "        df_selected = df[[\"Address\", \"Layout\", \"Rent\"]].copy()\n",
    "        df_selected.insert(0, \"Area Name\", sheet_name)  # Add Area Name column\n",
    "        # Append to the final DataFrame\n",
    "        final_df = pd.concat([final_df, df_selected], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"⚠ Skipping {sheet_name} due to missing columns!\")\n",
    "    final_df.loc[final_df[\"Area Name\"] == \"Fenway\", [\"Violent CrimeRate\", \"Overall CrimeRate\"]] = [0.2866, 1.86]\n",
    "    final_df.loc[final_df[\"Area Name\"] == \"Jamaica Plain\", [\"Violent CrimeRate\", \"Overall CrimeRate\"]] = [0.03, 0.28]\n",
    "    final_df.loc[final_df[\"Area Name\"] == \"Brookline\", [\"Violent CrimeRate\", \"Overall CrimeRate\"]] = [0.01, 0.25]\n",
    "    final_df.loc[final_df[\"Area Name\"] == \"Roxbury\", [\"Violent CrimeRate\", \"Overall CrimeRate\"]] = [0.38, 0.07]\n",
    "    final_df.loc[final_df[\"Area Name\"] == \"Dorchester\", [\"Violent CrimeRate\", \"Overall CrimeRate\"]] = [0.2840, 0.37]\n",
    "    final_df.loc[final_df[\"Area Name\"] == \"Mission Hill\", [\"Violent CrimeRate\", \"Overall CrimeRate\"]] = [0.03, 0.28]\n",
    "    final_df.loc[final_df[\"Area Name\"] == \"AllstonBrighton\", [\"Violent CrimeRate\", \"Overall CrimeRate\"]] = [0.13, 0.25]\n",
    "    final_df.loc[final_df[\"Area Name\"] == \"Medford\", [\"Violent CrimeRate\", \"Overall CrimeRate\"]] = [0.232, 0.14]\n",
    "    final_df.loc[final_df[\"Area Name\"] == \"DowntownSouth End\", [\"Violent CrimeRate\", \"Overall CrimeRate\"]] = [0.02, 0.14]\n",
    "    \n",
    "\n",
    "    final_df[[\"Bed\", \"Bath\"]] = final_df[\"Layout\"].str.extract(r'(\\d+)Bed\\s*(\\d+)Bath')\n",
    "\n",
    "    # Convert to numeric type\n",
    "    final_df[\"Bed\"] = pd.to_numeric(final_df[\"Bed\"], errors='coerce')\n",
    "    final_df[\"Bath\"] = pd.to_numeric(final_df[\"Bath\"], errors='coerce')\n",
    "# Check if final DataFrame is empty before saving\n",
    "final_df.drop(columns=[\"Layout\"], inplace=True)\n",
    "if not final_df.empty:\n",
    "    final_df.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ CSV file '{output_csv}' has been created successfully.\")\n",
    "else:\n",
    "    print(\"❌ No valid data found! Check your Excel file and column names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as 'merged_unique.csv' without duplicates.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both CSV files\n",
    "df1 = pd.read_csv(\"apartments_with_transit_cost.csv\")\n",
    "df2 = pd.read_csv(\"apartments_with_transit_cost.csv\")\n",
    "\n",
    "# Concatenate data from both files\n",
    "combined_df = pd.concat([df1, df2])\n",
    "\n",
    "# Remove duplicate entries\n",
    "unique_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "unique_df.to_csv(\"merged_unique.csv\", index=False)\n",
    "\n",
    "print(\"Merged file saved as 'merged_unique.csv' without duplicates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 7 fields in line 3378, saw 12\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlistings.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n",
      "File \u001b[1;32mc:\\Users\\avjay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\avjay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\avjay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\avjay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 7 fields in line 3378, saw 12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"listings.csv\")\n",
    "dictionary = {}\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i, \"Area Name\"] not in dictionary:\n",
    "        dictionary[df.loc[i, \"Area Name\"]] = {\"Total Listings\": 1, \"Average Rent\": df.loc[i, \"Rent\"]}\n",
    "    else:\n",
    "        dictionary[df.loc[i, \"Area Name\"]][\"Total Listings\"] += 1\n",
    "        dictionary[df.loc[i, \"Area Name\"]][\"Average Rent\"] += df.loc[i, \"Rent\"]\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Area Name               Address  Rent  Violent CrimeRate  Overall CrimeRate  \\\n",
      "0    Fenway   165 Hemenway Unit 5  2950             0.2866               1.86   \n",
      "1    Fenway  132 Hemenway Unit 10  3200             0.2866               1.86   \n",
      "2    Fenway  238 Hemenway Unit 11  4850             0.2866               1.86   \n",
      "3    Fenway  238 Hemenway Unit b5  3195             0.2866               1.86   \n",
      "4    Fenway  238 Hemenway Unit B8  3395             0.2866               1.86   \n",
      "\n",
      "   Bed  Bath  Northeastern University_transit  \\\n",
      "0  2.0   1.0                            196.0   \n",
      "1  2.0   1.0                             76.0   \n",
      "2  3.0   1.0                            170.0   \n",
      "3  2.0   1.0                            170.0   \n",
      "4  2.0   1.0                            170.0   \n",
      "\n",
      "   Northeastern University_driving  Boston University_transit_distance  \\\n",
      "0                              196                              3606.0   \n",
      "1                               76                              3703.0   \n",
      "2                              170                              3390.0   \n",
      "3                              170                              3390.0   \n",
      "4                              170                              3390.0   \n",
      "\n",
      "   Boston University_driving_distance  Boston College_transit_distance  \\\n",
      "0                                1491                          10488.0   \n",
      "1                                1371                           8425.0   \n",
      "2                                1798                          11478.0   \n",
      "3                                1798                          11478.0   \n",
      "4                                1798                          11478.0   \n",
      "\n",
      "   Boston College_driving_distance  Northeastern University_transit_cost  \\\n",
      "0                             7922                                   0.0   \n",
      "1                            12394                                   0.0   \n",
      "2                             7729                                   0.0   \n",
      "3                             7729                                   0.0   \n",
      "4                             7729                                   0.0   \n",
      "\n",
      "   Boston University_transit_cost  Boston College_transit_cost  \n",
      "0                             1.7                          2.4  \n",
      "1                             1.7                          2.4  \n",
      "2                             1.7                          5.8  \n",
      "3                             1.7                          5.8  \n",
      "4                             1.7                          5.8  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "df1 = pd.read_csv(\"apartments_with_transit_cost.csv\")  # Replace with actual file names\n",
    "df2 = pd.read_csv(\"apartments2_with_distances.csv\")\n",
    "\n",
    "# Standardize column names (strip spaces)\n",
    "df1.columns = df1.columns.str.strip()\n",
    "df2.columns = df2.columns.str.strip()\n",
    "\n",
    "# Define the required column order\n",
    "column_order = [\n",
    "    \"Area Name\", \"Address\", \"Rent\", \"Violent CrimeRate\", \"Overall CrimeRate\", \n",
    "    \"Bed\", \"Bath\", \"Northeastern University_transit\", \"Northeastern University_driving\",\n",
    "    \"Boston University_transit_distance\", \"Boston University_driving_distance\", \n",
    "    \"Boston College_transit_distance\", \"Boston College_driving_distance\", \n",
    "    \"Northeastern University_transit_cost\", \"Boston University_transit_cost\", \n",
    "    \"Boston College_transit_cost\"\n",
    "]\n",
    "\n",
    "# Find all unique columns across both DataFrames\n",
    "all_columns = set(df1.columns) | set(df2.columns)\n",
    "\n",
    "# Add any missing columns with NaN values\n",
    "for col in all_columns:\n",
    "    if col not in df1.columns:\n",
    "        df1[col] = None\n",
    "    if col not in df2.columns:\n",
    "        df2[col] = None\n",
    "\n",
    "# Ensure DataFrames have the specified column order\n",
    "df1 = df1[column_order]\n",
    "df2 = df2[column_order]\n",
    "\n",
    "# Concatenate DataFrames\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df.head())\n",
    "\n",
    "# Save to a new CSV file (optional)\n",
    "combined_df.to_csv(\"combined_file.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Area Name               Address  Rent  Violent CrimeRate  Overall CrimeRate  \\\n",
      "0    Fenway   165 Hemenway Unit 5  2950             0.2866               1.86   \n",
      "1    Fenway  132 Hemenway Unit 10  3200             0.2866               1.86   \n",
      "2    Fenway  238 Hemenway Unit 11  4850             0.2866               1.86   \n",
      "3    Fenway  238 Hemenway Unit b5  3195             0.2866               1.86   \n",
      "4    Fenway  238 Hemenway Unit B8  3395             0.2866               1.86   \n",
      "\n",
      "   Bed  Bath  Northeastern University_transit  \\\n",
      "0  2.0   1.0                            196.0   \n",
      "1  2.0   1.0                             76.0   \n",
      "2  3.0   1.0                            170.0   \n",
      "3  2.0   1.0                            170.0   \n",
      "4  2.0   1.0                            170.0   \n",
      "\n",
      "   Northeastern University_driving  Boston University_transit_distance  \\\n",
      "0                              196                              3606.0   \n",
      "1                               76                              3703.0   \n",
      "2                              170                              3390.0   \n",
      "3                              170                              3390.0   \n",
      "4                              170                              3390.0   \n",
      "\n",
      "   Boston University_driving_distance  Boston College_transit_distance  \\\n",
      "0                                1491                          10488.0   \n",
      "1                                1371                           8425.0   \n",
      "2                                1798                          11478.0   \n",
      "3                                1798                          11478.0   \n",
      "4                                1798                          11478.0   \n",
      "\n",
      "   Boston College_driving_distance  Northeastern University_transit_cost  \\\n",
      "0                             7922                                   0.0   \n",
      "1                            12394                                   0.0   \n",
      "2                             7729                                   0.0   \n",
      "3                             7729                                   0.0   \n",
      "4                             7729                                   0.0   \n",
      "\n",
      "   Boston University_transit_cost  Boston College_transit_cost  \n",
      "0                             1.7                          2.4  \n",
      "1                             1.7                          2.4  \n",
      "2                             1.7                          5.8  \n",
      "3                             1.7                          5.8  \n",
      "4                             1.7                          5.8  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "final_df = pd.read_csv(\"combined_file.csv\")  # Replace with your actual file\n",
    "\n",
    "# Standardize column names (strip spaces)\n",
    "final_df.columns = final_df.columns.str.strip()\n",
    "\n",
    "# Define the crime rate mapping\n",
    "crime_rate_mapping = {\n",
    "    \"Fenway\": [0.2866, 1.86],\n",
    "    \"Jamaica Plain\": [0.03, 0.28],\n",
    "    \"Brookline\": [0.01, 0.25],\n",
    "    \"Roxbury\": [0.38, 0.07],\n",
    "    \"Dorchester\": [0.2840, 0.37],\n",
    "    \"Mission hill\": [0.03, 0.28],\n",
    "    \"AllstonBrighton\": [0.13, 0.25],\n",
    "    \"Medford\": [0.232, 0.14],\n",
    "    \"DowntownSouth End\": [0.02, 0.14],\n",
    "    \"Brighton\": [0.13, 0.25],\n",
    "    \"Allston\": [0.13, 0.25],\n",
    "}\n",
    "\n",
    "# Update the DataFrame with new crime rates\n",
    "for area, rates in crime_rate_mapping.items():\n",
    "    final_df.loc[final_df[\"Area Name\"] == area, [\"Violent CrimeRate\", \"Overall CrimeRate\"]] = rates\n",
    "\n",
    "# Display updated DataFrame\n",
    "print(final_df.head())\n",
    "\n",
    "# Save the updated DataFrame (optional)\n",
    "final_df.to_csv(\"updated_crime_rates.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
